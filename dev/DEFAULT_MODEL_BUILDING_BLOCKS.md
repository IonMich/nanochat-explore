# Default Nanochat Model Building Blocks

_Generated from `dev/architecture_blocks.source.json` on 2026-02-06 19:16:25Z. Do not edit this file directly._

Default checkpoint path:
- `python -m scripts.tok_train`
- `python -m scripts.base_train`

## 1) Default Model Instantiation

- The default base model checkpoint is created through `scripts/base_train.py`.
- Default model args are `depth=20`, `aspect-ratio=64`, `head-dim=128`, `max-seq-len=2048`, `window-pattern=SSSL`.
- With these defaults, the model shape is `n_layer=20`, `n_embd=1280`, `n_head=10`, `n_kv_head=10`, `sequence_len=2048`.

Code locations:
- `scripts/base_train.py:40-53`
- `scripts/base_train.py:125-136`
- `scripts/base_train.py:142-147`

## 2) Tokenizer Used By Default

- The default tokenizer training path uses `RustBPETokenizer` with vocab size `32768`.
- Runtime loading in base training uses `get_tokenizer()` which returns `RustBPETokenizer.from_directory(...)`.
- Special conversation tokens are part of the tokenizer definition and are shared with downstream chat phases.

Code locations:
- `scripts/tok_train.py:16-20`
- `nanochat/tokenizer.py:13-25`
- `nanochat/tokenizer.py:163-190`
- `nanochat/tokenizer.py:390-395`

## 3) Embedding And Positional Signal

- Token embeddings live in `transformer.wte`, and there is no learned positional embedding table.
- Position information is carried by RoPE buffers (`cos`, `sin`) that are precomputed and sliced in forward.
- The input embedding is RMS-normalized before entering the first transformer block.

Code locations:
- `nanochat/gpt.py:163-167`
- `nanochat/gpt.py:243-258`
- `nanochat/gpt.py:391-402`

## 4) Transformer Block Skeleton

- Each block has two residual branches: attention and MLP, each with pre-norm (`norm(x)`).
- Before each block, the running residual stream is mixed with the initial embedding `x0` using learned per-layer scalars (`resid_lambdas`, `x0_lambdas`).

Code locations:
- `nanochat/gpt.py:134-143`
- `nanochat/gpt.py:172-173`
- `nanochat/gpt.py:403-407`

## 5) Attention Core

- Attention uses bias-free `q`, `k`, `v`, `proj` linear layers.
- RoPE is applied to `q` and `k`, then QK norm is applied, and attention is computed with the unified flash attention wrapper.
- Sliding-window attention is set per layer by `window_pattern`, with the final layer always forced to full context.

Code locations:
- `nanochat/gpt.py:59-75`
- `nanochat/gpt.py:91-101`
- `nanochat/gpt.py:260-287`

## 6) Value Embeddings And Gating

- This implementation includes value embeddings (`value_embeds`) on alternating layers and always includes the final layer.
- When present, each value embedding is injected into attention values through a learned sigmoid gate.

Code locations:
- `nanochat/gpt.py:47-49`
- `nanochat/gpt.py:175-177`
- `nanochat/gpt.py:85-90`
- `nanochat/gpt.py:405-406`

## 7) MLP Stack

- The MLP is a two-layer bias-free feedforward module with expansion ratio 4x.
- Activation is ReLU-squared (`relu(x)^2`) instead of GELU.

Code locations:
- `nanochat/gpt.py:121-131`

## 8) Output Head And Training Loss

- After all blocks, the stream is RMS-normalized and projected through an untied LM head.
- Logits are cropped back to vocab size (after optional padded vocab allocation) and softcapped with `15 * tanh(x/15)`.
- Training uses next-token cross-entropy over shifted targets from the dataloader.

Code locations:
- `nanochat/gpt.py:407-419`
- `nanochat/gpt.py:167`

## 9) Parameter Initialization

- `model.init_weights()` is the initialization entry used in default training.
- Embedding and unembedding are initialized separately from transformer matrices.
- Attention and MLP output projections are zero-initialized, and residual scalar parameters are explicitly initialized.

Code locations:
- `scripts/base_train.py:146-147`
- `nanochat/gpt.py:204-221`
- `nanochat/gpt.py:227-230`

## 10) Attention Runtime Backend (FA3 Or SDPA)

- Runtime attention backend selection is centralized in `nanochat/flash_attention.py`.
- If Flash Attention 3 is available on Hopper (`sm90`), it is used. Otherwise, it falls back to PyTorch SDPA.
- This affects runtime kernels, not checkpoint parameter structure.

Code locations:
- `nanochat/flash_attention.py:23-42`
- `nanochat/flash_attention.py:99-120`
- `nanochat/flash_attention.py:123-169`

## 11) Default Dataloader And Target Shift

- Default base training uses the BOS-aligned best-fit dataloader variant.
- Rows are packed as `T+1`, then shifted into `inputs=row[:-1]` and `targets=row[1:]` for next-token prediction.

Code locations:
- `scripts/base_train.py:335-338`
- `nanochat/dataloader.py:97-100`
- `nanochat/dataloader.py:153-160`

## 12) What Gets Saved In A Default Checkpoint

- Default checkpoint writing happens inside the training loop with `save_checkpoint(...)`.
- The model state dict, optimizer state, and metadata are saved. Metadata includes `model_config` and `user_config`.
- The `model_config` payload is the architecture source of truth used later when loading checkpoints.

Code locations:
- `scripts/base_train.py:444-466`

## 13) Default Parameter Count (Depth 20 Path)

- Using default `depth=20`, `aspect-ratio=64`, `head-dim=128`, and tokenizer vocab `32768`, total parameters are `896,535,720`.
- Breakdown: `wte=41,943,040`, `value_embeds=419,430,400`, `lm_head=41,943,040`, `transformer_matrices=393,219,200`, `scalars=40`.
- Counting groups are implemented in `num_scaling_params()`.

Code locations:
- `nanochat/gpt.py:319-346`

## 14) Explicitly Not In The Default Path

- The HuggingFace tokenizer wrapper exists but is not used by default base training path.
- FP8 conversion exists behind `--fp8` and is disabled by default.
- Non-equal GQA head settings are possible in config space, but default path sets `n_kv_head == n_head`.

Code locations:
- `nanochat/tokenizer.py:39-59`
- `scripts/base_train.py:46-47`
- `scripts/base_train.py:164-184`
- `scripts/base_train.py:134`
